{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 80403\n",
      "Shape of Data Tensor: (12688, 1024)\n",
      "Shape of Label Tensor: (12688, 2)\n",
      "Total 71291 word vectors in Glove 6B 300d.\n",
      "Simplified convolutional neural network\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 1024, 300)         24121200  \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 1020, 128)         192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 204, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 204, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 200, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 36, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 24,494,194\n",
      "Trainable params: 24,494,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9196 samples, validate on 2860 samples\n",
      "Epoch 1/5\n",
      "9196/9196 [==============================] - 819s 89ms/step - loss: 0.7084 - acc: 0.5004 - val_loss: 0.6962 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50000, saving model to model_cnn.hdf5\n",
      "- val_f1: 0.3333 - val_precision: 0.2500 - val_recall: 0.5000\n",
      "Epoch 2/5\n",
      "9196/9196 [==============================] - 818s 89ms/step - loss: 0.6945 - acc: 0.5107 - val_loss: 0.6727 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50000 to 0.62273, saving model to model_cnn.hdf5\n",
      "- val_f1: 0.5696 - val_precision: 0.7425 - val_recall: 0.6227\n",
      "Epoch 3/5\n",
      "9196/9196 [==============================] - 723s 79ms/step - loss: 0.6657 - acc: 0.5947 - val_loss: 0.5086 - val_acc: 0.7878\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.62273 to 0.78776, saving model to model_cnn.hdf5\n",
      "- val_f1: 0.7871 - val_precision: 0.7916 - val_recall: 0.7878\n",
      "Epoch 4/5\n",
      "9196/9196 [==============================] - 842s 92ms/step - loss: 0.6222 - acc: 0.6516 - val_loss: 0.5076 - val_acc: 0.7846\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.78776\n",
      "- val_f1: 0.7834 - val_precision: 0.7913 - val_recall: 0.7846\n",
      "Epoch 5/5\n",
      "9196/9196 [==============================] - 797s 87ms/step - loss: 0.5897 - acc: 0.6798 - val_loss: 0.4730 - val_acc: 0.8035\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78776 to 0.80350, saving model to model_cnn.hdf5\n",
      "- val_f1: 0.8034 - val_precision: 0.8041 - val_recall: 0.8035\n",
      "<keras.callbacks.callbacks.History object at 0x1c3c8b1940>\n",
      "f1_score-->> [0.3333333333333333, 0.5695783920246319, 0.7870705998787444, 0.7833797945737732, 0.8033980535022673]\n",
      "precision---->> [0.25, 0.7425075492209203, 0.7915502957641735, 0.7912607269182343, 0.8041056367980463]\n",
      "recalls----->> [0.5, 0.6227272727272727, 0.7877622377622377, 0.7846153846153846, 0.8034965034965035]\n",
      "[array([[0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32), array([[1., 0.],\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32)]\n",
      "[array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [0., 1.],\n",
      "       [0., 1.]], dtype=float32)]\n",
      "CNN Accuracy_Score = 0.835443\n",
      "CNN Precision = 0.837729\n",
      "CNN Recall = 0.835443\n",
      "CNN F1-Score  = 0.835164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano' # Why theano why not\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint,Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "#%matplotlib inline\n",
    "\n",
    "def Get_Accuracy(y_true, y_pred): #Accuracy 准确率：分类器正确分类的样本数与总样本数之比 \n",
    "#    accuracy = accuracy_score(y_true,y_pred,normalize = False) \n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def Get_Precision_score(y_true, y_pred): #Precision：精准率 正确被预测的正样本(TP)占所有被预测为正样本(TP+FP)的比例. \n",
    "    precision = precision_score(y_true,y_pred,average='weighted')  \n",
    "    return precision\n",
    "\n",
    "def Get_Recall(y_true, y_pred): #Recall 召回率 正确被预测的正样本(TP)占所有真正 正样本(TP+FN)的比例.  \n",
    "    Recall = recall_score(y_true,y_pred,average='weighted')  \n",
    "    return Recall \n",
    " \n",
    "def Get_f1_score(y_true, y_pred): #F1-score: 精确率(precision)和召回率(Recall)的调和平均数  \n",
    "    f1_score1 = f1_score(y_true,y_pred,average='weighted')  \n",
    "    return f1_score1\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def __init__(self):\n",
    "        self.predict = []\n",
    "        self.target = []\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "#        self.confusion_matrixs = np.array()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict,average='weighted')\n",
    "        _val_recall = recall_score(val_targ, val_predict,average='weighted')\n",
    "        _val_precision = precision_score(val_targ, val_predict,average='weighted')\n",
    "#         array,confusion_matrixs = confusion_matrix(val_targ,val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        self.predict.append(val_predict)\n",
    "        self.target.append(val_targ)\n",
    "#         print('----------------------------------------------')\n",
    "#         print(array,confusion_matrixs)\n",
    "#         print('----------------------------------------------')\n",
    "        print('- val_f1: %.4f - val_precision: %.4f - val_recall: %.4f'%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1024\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DROP_OUT_LAYER = 0.2\n",
    "\n",
    "# reading data\n",
    "df1 = pd.read_excel('train.xls')\n",
    "df1 = df1.dropna()\n",
    "df1 = df1.reset_index(drop=True)\n",
    "SPLIT_LINE = df1.Deal_editorial.shape[0]\n",
    "df2 = pd.read_excel('valid.xls')\n",
    "df2 = df2.dropna()\n",
    "df2 = df2.reset_index(drop=True)\n",
    "TEST_LINE = df2.Deal_editorial.shape[0]\n",
    "df3 = pd.read_excel('test.xls')\n",
    "df3 = df3.dropna()\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df = df1.append(df2).append(df3)\n",
    "# print('Shape of dataset ',df.shape)\n",
    "# print(df.columns)\n",
    "\n",
    "macronum=sorted(set(df['Deal_status']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "df['Deal_status']=df['Deal_status'].apply(fun)\n",
    "\n",
    "texts = []\n",
    "for i in range(len(list(df['Deal_editorial']))):\n",
    "    texts.append(list(df['Deal_editorial'])[i].replace(\"\\n\",\"\"))\n",
    "labels = []\n",
    "\n",
    "# for idx in range(df.Deal_editorial.shape[0]):\n",
    "#     text = BeautifulSoup(df.Deal_editorial[idx])\n",
    "#     texts.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "for idx in df['Deal_status']:\n",
    "    labels.append(idx)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of Data Tensor:', data.shape)\n",
    "print('Shape of Label Tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:SPLIT_LINE]\n",
    "y_train = labels[:SPLIT_LINE]\n",
    "x_val = data[SPLIT_LINE:SPLIT_LINE+TEST_LINE]\n",
    "y_val = labels[SPLIT_LINE:SPLIT_LINE+TEST_LINE]\n",
    "x_test = data[SPLIT_LINE+TEST_LINE:]\n",
    "y_test = labels[SPLIT_LINE+TEST_LINE:]\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../GloVe/vectors.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 300d.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_pool1 = Dropout(DROP_OUT_LAYER)(l_pool1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_pool2 = Dropout(DROP_OUT_LAYER)(l_pool2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_pool3 = Dropout(DROP_OUT_LAYER)(l_pool3)\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_flat = Dropout(DROP_OUT_LAYER)(l_flat)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='rmsprop',\n",
    "                  optimizer='adamax',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n",
    "metrics = Metrics()\n",
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=5, batch_size=8,callbacks=[cp,metrics])\n",
    "\n",
    "print(history)\n",
    "#print(\"confusion--->>\",metrics.confusion_matrixs)\n",
    "print(\"f1_score-->>\",metrics.val_f1s)\n",
    "print(\"precision---->>\",metrics.val_precisions)\n",
    "print(\"recalls----->>\",metrics.val_recalls)\n",
    "\n",
    "print(metrics.predict)\n",
    "print(metrics.target)\n",
    "\n",
    "y_predict1 = model.predict(x_test)\n",
    "\n",
    "y_predict = (y_predict1>0.5)\n",
    "accuracy = Get_Accuracy(y_test,y_predict)\n",
    "print(\"CNN Accuracy_Score = %f\"%accuracy) \n",
    "precision = Get_Precision_score(y_test,y_predict)\n",
    "print(\"CNN Precision = %f\"%precision)\n",
    "recall = Get_Recall(y_test,y_predict)\n",
    "print(\"CNN Recall = %f\"%recall) \n",
    "f1_score1 = Get_f1_score(y_test,y_predict)\n",
    "print(\"CNN F1-Score  = %f\"%f1_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [False,  True],\n",
       "       [False,  True],\n",
       "       ...,\n",
       "       [False,  True],\n",
       "       [False,  True],\n",
       "       [False,  True]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
